{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb48cc8c-51e2-44ff-82dd-6036b819f637",
      "metadata": {
        "id": "bb48cc8c-51e2-44ff-82dd-6036b819f637"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverGumSecurity/AILabs/blob/main/016_Fundamentals/PhishingModel.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a527797b-bcb0-4a69-907f-f545e604296c",
      "metadata": {
        "id": "a527797b-bcb0-4a69-907f-f545e604296c"
      },
      "source": [
        "## Hugging Face: Phishing Classification Model\n",
        "\n",
        "In this lab/demo, we are going to use a pre-trained AI classification model for email phishing. This model is able to\n",
        "injest text or html data, and classify it as either \"benign\" or \"phishing\". This pre-trained model leverages BERT from Google\n",
        "which was trained in 2018. The concept of leveraging BERT, a natural language processing (NLP) model, and extending the training for\n",
        "a specific task is known as **transfer learning**.\n",
        "\n",
        "It is important to understand that a tremendous amount of the statistical and mathematical operations needed to build and use AI models are being abstracted away when you use a pre-trained model interface/API. This is actually a good thing for us, as training your own model is a specialized skill that requires some understanding of the underlying statistical theory, and even better, an understanding of the underlying linear algebra that is commonly used for all vector based computational neural networks.\n",
        "\n",
        "A common concept in natural language processing (NLP) is the idea of **tokenizing**. In it's simplest form, **tokenizing** is a process whereby string tokens (words or individual characters) are turned into mathematical vectors, otherwize known as matricies for ease of numerical computation. Since either words, or sentences do not have a fixed length, and a fixed two dimensional array (matrix) does have a limit, then we often are in the situation of truncating or padding data when performing the **tokenizing** process.\n",
        "\n",
        "In this first Jupyter notebook cell, we use the huggingface_hub, and transformers Python modules. Both are designed to facilitate the download and use of pre-trained AI models. Some points to note as follows:\n",
        "\n",
        "* [**huggingface_hub.snapshot_download()**](https://huggingface.co/docs/huggingface_hub/v0.24.2/en/package_reference/file_download#huggingface_hub.snapshot_download) is used to download a snapshot of the entire AI model repository.\n",
        "* [**transformers.AutoTokenizer.from_pretrained()**](https://huggingface.co/docs/transformers/v4.43.0/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) is used to create a text tokenizer object from a pre-trained model.\n",
        "* [**transformers.AutoModelForSequenceClassification.from_pretrained()**](https://huggingface.co/docs/transformers/v4.43.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) is used to create a model object from which predictions can be made.\n",
        "* [**transformers.pipeline()**](https://huggingface.co/docs/transformers/en/main_classes/pipelines) is a very high level powerful functional abstraction that allows you to feed input data to an AI model for a prediction without the complex statistical API calls which are being used within the pipeline() abstraction.\n",
        "    * Please note the function argument **device='mps'** is included so that the pipeline leverages the MacOS MPS GPU core for processing.\n",
        "\n",
        "### **bert-finetuned-phishing** Model Accuracy Claims\n",
        "\n",
        "According to the hugging face published document, this model achieves the following results on the evaluation dataset:\n",
        "\n",
        "* Loss: 0.1953\n",
        "* Accuracy: 0.9717\n",
        "* Precision: 0.9658\n",
        "* Recall: 0.9670\n",
        "* False Positive Rate: 0.0249\n",
        "\n",
        "### Our Goals\n",
        "\n",
        "Analyze the original training dataset, and evaluate from an alternative dataset whether this accuracy claim seems\n",
        "correct to us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86533022",
      "metadata": {
        "id": "86533022",
        "outputId": "11b86cc4-0a55-46c7-a9b2-b8eb70574989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret HF_APIKEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe9a62b7d823>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'google.colab'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mHF_APIKEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_APIKEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'.hfkey'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HF_APIKEY does not exist."
          ]
        }
      ],
      "source": [
        "#################################################\n",
        "## Hugging Face Phishing Model Demo\n",
        "## Author: Joff Thyer, Copyright (c) July 2024\n",
        "#################################################\n",
        "import huggingface_hub\n",
        "import transformers\n",
        "import pathlib\n",
        "import torch\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# test if datasets is available, if not install it!\n",
        "if 'datasets' not in sys.modules:\n",
        "    !pip install datasets\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Try to determine Hugging Face API key\n",
        "# You must either store your hugging face key on local disk\n",
        "# or in Google CoLab Secrets\n",
        "HF_APIKEY = ''\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import userdata\n",
        "    HF_APIKEY = userdata.get('WWHF_Token')\n",
        "else:\n",
        "    with open(pathlib.Path.home() / '.hfkey') as hf:\n",
        "        HF_APIKEY = hf.read().strip()\n",
        "if not HF_APIKEY:\n",
        "    print('[-] ERROR: Cannot continue without Hugging Face API Key')\n",
        "    sys.exit(0)\n",
        "\n",
        "# detect GPU as needed\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "print(f'[*] Data will be processed on [{device}] architecture.')\n",
        "\n",
        "model_name = \"ealvaradob/bert-finetuned-phishing\"\n",
        "dpath = huggingface_hub.snapshot_download(repo_id=model_name, token=HF_APIKEY)\n",
        "print(f'Model downloaded to: {dpath}')\n",
        "\n",
        "# now setup a tokenizer, a model object, and a prediction object\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device=device)\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "predict = transformers.pipeline('text-classification', model=model, tokenizer=tokenizer, device=device, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9b8bf0-3f9d-4031-9ff9-c60131b39f9b",
      "metadata": {
        "id": "fb9b8bf0-3f9d-4031-9ff9-c60131b39f9b"
      },
      "outputs": [],
      "source": [
        "# original training dataset loaded here.\n",
        "dataset = load_dataset(\"ealvaradob/phishing-dataset\", \"combined_reduced\", trust_remote_code=True)\n",
        "tempdf = dataset['train'].to_pandas()\n",
        "train, test = train_test_split(tempdf, test_size=0.10, random_state=42)\n",
        "test.replace({'label': 0}, 'benign', inplace=True)\n",
        "test.replace({'label': 1}, 'phishing', inplace=True)\n",
        "\n",
        "train, test = Dataset.from_pandas(train, preserve_index=False), Dataset.from_pandas(test, preserve_index=False)\n",
        "print(f'train dataset: {train.shape}')\n",
        "print(f'test dataset: {test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afce370b-e36a-402f-8144-527ea865c0b1",
      "metadata": {
        "id": "afce370b-e36a-402f-8144-527ea865c0b1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def batch_predictor(ds, infield, match, outfield, batch_size=256, labels=False):\n",
        "    res = []\n",
        "    data = DataLoader(ds, batch_size=batch_size)\n",
        "    i = 0\n",
        "    for batch in data:\n",
        "        print(f'\\r[*] Predicting [{i:05d}]', flush=True, end='')\n",
        "        p = [t[outfield] == match for t in predict(batch[infield])]\n",
        "        res.extend(p)\n",
        "        i += batch_size\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0020aa-a045-49b8-8ff4-4c6c66d58b10",
      "metadata": {
        "id": "9c0020aa-a045-49b8-8ff4-4c6c66d58b10"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table used to evaluate the performance of a classification algorithm. It is particularly useful for understanding how well a classifier is distinguishing between classes, especially in binary classification. The matrix compares the actual target values with the values predicted by the model. Here’s a breakdown of the components of a confusion matrix for binary classification.\n",
        "\n",
        "Components:\n",
        "* True Positive (TP): The number of instances correctly predicted as positive.\n",
        "* True Negative (TN): The number of instances correctly predicted as negative.\n",
        "* False Positive (FP): The number of instances incorrectly predicted as positive (Type I error).\n",
        "* False Negative (FN): The number of instances incorrectly predicted as negative (Type II error).\n",
        "\n",
        "In the case of phishing classification, we have a binary classifier, thus the components break down as:\n",
        "* True Positive (TP): correctly classified as spam.\n",
        "* True Negative (TN): correctly classified as not spam.\n",
        "* False Positive (FP): incorrectly classified as spam.\n",
        "* False Negative (FN): incorrectly classified as not spam.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60c9433-f401-4e70-b3b7-4b3340bdbf10",
      "metadata": {
        "id": "d60c9433-f401-4e70-b3b7-4b3340bdbf10"
      },
      "outputs": [],
      "source": [
        "# confusion matrix example using test/train split.\n",
        "y_test = [t['label'] == 'phishing' for t in test]\n",
        "y_pred = batch_predictor(test, 'text', 'phishing', 'label', batch_size=256)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "accuracy = (cm[0][0] + cm[1][1]) / test.num_rows\n",
        "print(f'\\r\\n[*] Calculated Model Accuracy = {accuracy:02f}')\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
        "_ = disp.plot(cmap=plt.cm.Blues)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9458d15b-50b4-4fbb-b498-f07b5ede037a",
      "metadata": {
        "id": "9458d15b-50b4-4fbb-b498-f07b5ede037a"
      },
      "source": [
        "## Pandas\n",
        "\n",
        "The Python module **Pandas** is an important and powerful module being used in data science.\n",
        "Pandas provides two core concepts as Python classes known as a **Dataframe**, and a **Series**.\n",
        "For natural language processing tasks, the **Dataframe** class is very commonly used.\n",
        "In the below cell we are reading a dataset from a comma delimited file, replacing some column names, and then collecting some sample data from the data frame in order to make AI model predictions from that sample data.\n",
        "\n",
        "* we import pandas and a common convention in data science is to alias it as **pd** in Python scripts\n",
        "* in the code below we create two dataframes, \"**source**\" contains the full phishing email comma delimited dataset while \"**df**\" contains a subset that is sampled from the source.\n",
        "* [**pd.read_csv()**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) is commonly used to read and process comma delimited data. It has many options as you can read in the documentation link, and it nicely allows you to use either a file name or URL to read data from.\n",
        "* [**pd.dropna()**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) is used to drop any rows with NULL data in the dataframe. Using **inplace=True** allows us to mutate the dataframe inplace rather than having to assign the result to a new object.\n",
        "* [**pd.replace()**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) is a very useful function for searching and replace data in an entire dataframe. It can perform simple string replacement or even use regular expressions if you have the need for more complex operations.\n",
        "    * In our use case, the AI model we are using on prediction returns either the string \"**phishing**\" or \"**benign**\".\n",
        "    * In the phishing labelled dataset, the labels are either \"**Phishing Email**\" or \"**Safe Email**\".\n",
        "    * It is desirable to have the same nomenclature thus we use *replace* to fix this discrepency.\n",
        "* [**pd.sample()**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) allows us to take a sampling from a dataframe and return another dataframe. We choose a sample size that is manageable in terms of the real time execution time elapsed when making AI model predictions.\n",
        "    * AI model predictions require linear algebraic computation which is always best performed on a vector processor design architecture such as Nvidia (CUDA based) Graphics Processing Units (GPU), or the [MacBook Pro M3 Meta Performance Shader (MPS) core](https://developer.apple.com/videos/play/tech-talks/111375/).\n",
        "    * Pure CPU based calculation alone does work but is orders of magnitude slower.\n",
        "* [**pd.assign()**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html) is used to create a new column in the dataframe. In our use case this new column will become the prediction from the AI model so that we can store the prediction back to the same dataframe. We use a Python lambda function to create the number of rows in the new column to be the same as the length of the sampled dataframe structure.\n",
        "* [**pd.shape**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) is an attribute of the dataframe class that contains the **(rows, columns)** in the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ffb45f-befd-44f1-9774-e42e029ef15b",
      "metadata": {
        "id": "48ffb45f-befd-44f1-9774-e42e029ef15b"
      },
      "outputs": [],
      "source": [
        "source = pd.read_csv('https://github.com/RiverGumSecurity/Datasets/raw/main/Kaggle/Phishing_Email.csv.gz')\n",
        "source.dropna(inplace=True)\n",
        "source = source.replace('Phishing Email', 'phishing')\n",
        "source = source.replace('Safe Email', 'benign')\n",
        "\n",
        "sample_size = 6000\n",
        "df = source.sample(sample_size)\n",
        "df = df.assign(Prediction = lambda x: [None] * len(df))\n",
        "df = df.assign(Score = lambda x: [None] * len(df))\n",
        "\n",
        "print(f'The shape of the entire phishing dataset is: {source.shape}')\n",
        "print(f'The shape of the sampled data frame is: {df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccac07b1-274b-4efd-b782-a7b353b51ca6",
      "metadata": {
        "id": "ccac07b1-274b-4efd-b782-a7b353b51ca6"
      },
      "source": [
        "## Making the AI Model do some predictive work!\n",
        "\n",
        "This is where the rubber meets the road.  In the code below, we are looping through all of the data in our sampled \"**df**\" dataframe\n",
        "and making some predictions using the \"**predict**\" pipeline object we created at the start of this notebook.\n",
        "A few highlights to point out here:\n",
        "* the logic within the loop uses an arbritrary random integer to print out a progress count of sorts. It is meaningless with regard to the actual prediction\n",
        "* the \"**predict**\" object is passed the actual textual data and returns a Python dictionary inside a Python list. The dictionary has both a \"**score**\" and a \"**label**\" key in it. Since the label is a binary classification of *phishing* or *benign*, we only care about storing this result of the prediction using the \"**label**\" key to retrieve it.\n",
        "* the Pandas [**pd.iat()**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iat.html) method is used to assign the resulting prediction label to a specific row number in the dataframe.\n",
        "* the Python exception logic allows us to continue making predictions if there is some unknown failure at any specific row. It also allows us to catch the **KeyboardInterrupt** exception so that we can terminate the prediction loop early if we feel it is taking too long.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62ef117-d927-430c-a470-facf32eadc51",
      "metadata": {
        "id": "b62ef117-d927-430c-a470-facf32eadc51"
      },
      "outputs": [],
      "source": [
        "# Make Model Predictions\n",
        "spin = r'\\-/|+'\n",
        "mm = 0\n",
        "\n",
        "batch = []\n",
        "batch_size = 128\n",
        "for i, row in enumerate(df['Email Text']):\n",
        "    try:\n",
        "        if i > 0 and not len(batch) % batch_size:\n",
        "            ps = predict(batch)\n",
        "            j = i - batch_size\n",
        "            for p in ps:\n",
        "                df['Prediction'].iat[j] = p['label']\n",
        "                df['Score'].iat[j] = p['score']\n",
        "                if df['Prediction'].iat[j] != df['Email Type'].iat[j]:\n",
        "                    mm += 1\n",
        "                j += 1\n",
        "            print(f'\\r[{spin[i % len(spin)]}] Processed {i} rows of data.', end='', flush=True)\n",
        "            batch = []\n",
        "\n",
        "        # append the text into the batch list\n",
        "        batch.append(str(row))\n",
        "    except KeyboardInterrupt:\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        break\n",
        "print(f'\\r\\n[+] Number of prediction / label mismatches: {mm}')\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f5835c-20d0-4e02-910c-573a78f37c21",
      "metadata": {
        "id": "33f5835c-20d0-4e02-910c-573a78f37c21"
      },
      "source": [
        "## Visualization with MatPlotLib\n",
        "\n",
        "Being able to produce a good plot/chart to visualize results is critical for data science work.\n",
        "Python's open source **matplotlib** does a great job for us in this area, and works very well\n",
        "within Jupyter notebook. There are two basic modes you can use **matplotlib** in, *notebook* mode, or *inline* mode. Of these,\n",
        "using it *inline* is the default and easiest method. In full *notebook* mode, there is ability to use interactive charts\n",
        "however it requires tighter Python module integration and more software dependencies.\n",
        "\n",
        "In this example, we create a single **figure**, and then use the **subplot** method to create two different\n",
        "plots within the same figure so that we can compare the results side by side. Below is a list of methods used with links to\n",
        "the related documentation.\n",
        "\n",
        "* [**plt.figure()**](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html)\n",
        "* [**plt.subplot()**](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html)\n",
        "    * Note: subplot() arguments are \"rows\", \"cols\", and \"index\".\n",
        "    * in the example below, we create 1 x 2 subplots.\n",
        "* [**plt.title()**](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.title.html)\n",
        "* [**plt.xlabel()**](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html)\n",
        "* [**plt.ylabel()**](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html)\n",
        "* [**pandas.DataFrame.plot()**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd92d91-e61b-4cef-8729-2f17e61676a6",
      "metadata": {
        "id": "cfd92d91-e61b-4cef-8729-2f17e61676a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# this is the default mode however we include for good measure.\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(1, figsize=(10, 8))\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "plt.title('Labelled Frequency Counts')\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "plt.title('Predicted Frequency Counts')\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Frequency')\n",
        "_ = source['Email Type'].value_counts().plot(ax=ax1, kind='bar', color=['g', 'r'], label='SourceData', grid=True)\n",
        "\n",
        "# create a new pandas series containing our counts\n",
        "df2 = df.groupby(by = 'Prediction', as_index=True)['Prediction'].count()\n",
        "df2['mismatch'] = mm\n",
        "_ = df2.plot(ax=ax2, kind='bar', label='Prediction', color=['g', 'r', 'b'], grid=True, yticks=range(0, df2.max(), 100))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57209edf-0093-478b-ae49-63ae39811e72",
      "metadata": {
        "id": "57209edf-0093-478b-ae49-63ae39811e72"
      },
      "outputs": [],
      "source": [
        "# confusion matrix example using sampled data.\n",
        "y_test = [t == 'phishing' for t in df['Prediction']]\n",
        "y_pred = [t == 'phishing' for t in df['Email Type']]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "accuracy = (cm[0][0] + cm[1][1]) / len(y_test)\n",
        "print(f'Calculated Model Accuracy = {accuracy:02f}')\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
        "_ = disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebbe0820-77ad-4f5d-a66d-0a484eb3efe9",
      "metadata": {
        "id": "ebbe0820-77ad-4f5d-a66d-0a484eb3efe9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}